{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a098b651",
   "metadata": {},
   "source": [
    "Step 1: Install & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b33924",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torchvision matplotlib\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import json\n",
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.models.detection.ssd import SSD300_VGG16_Weights\n",
    "from torchvision.transforms import v2\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f095f7c",
   "metadata": {},
   "source": [
    "Step 2: Prepare COCO Subset Dataset (Image + Annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943368e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SSDDataset(Dataset):\n",
    "    def __init__(self, img_dir, ann_file, transform=None):\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        with open(ann_file, 'r') as f:\n",
    "            coco = json.load(f)\n",
    "\n",
    "        self.images = coco['images']\n",
    "        self.annotations = coco['annotations']\n",
    "        self.categories = coco['categories']\n",
    "        self.img_id_to_ann = {}\n",
    "        for ann in self.annotations:\n",
    "            self.img_id_to_ann.setdefault(ann['image_id'], []).append(ann)\n",
    "\n",
    "        self.cat_id_to_index = {cat['id']: idx for idx, cat in enumerate(self.categories)}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_info = self.images[idx]\n",
    "        img_path = os.path.join(self.img_dir, img_info['file_name'])\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        for ann in self.img_id_to_ann.get(img_info['id'], []):\n",
    "            x, y, w, h = ann['bbox']\n",
    "            boxes.append([x, y, x + w, y + h])\n",
    "            labels.append(self.cat_id_to_index[ann['category_id']])\n",
    "\n",
    "        target = {\n",
    "            \"boxes\": torch.as_tensor(boxes, dtype=torch.float32),\n",
    "            \"labels\": torch.as_tensor(labels, dtype=torch.int64)\n",
    "        }\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)  # <- fix here\n",
    "\n",
    "        return image, target\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09849b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "edee5e8a",
   "metadata": {},
   "source": [
    "Step 3: Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bab5ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models.detection.transform import GeneralizedRCNNTransform\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "train_dir = r\"C:/Users/admin/Downloads/Code/ObjectDetection/coco_subset/train2017\"\n",
    "val_dir = r\"C:/Users/admin/Downloads/Code/ObjectDetection/coco_subset/val2017\"\n",
    "train_ann = r\"C:/Users/admin/Downloads/Code/ObjectDetection/coco_subset/annotations/instances_train2017.json\"\n",
    "val_ann = r\"C:/Users/admin/Downloads/Code/ObjectDetection/coco_subset/annotations/instances_val2017.json\"\n",
    "\n",
    "# Basic transforms compatible with older torchvision\n",
    "ssd_transform = transforms.Compose([\n",
    "    transforms.Resize((300, 300)),                     # Resize to SSD input size\n",
    "    transforms.ToTensor(),                             # Convert to tensor\n",
    "    transforms.ConvertImageDtype(torch.float32)        # Normalize to float32\n",
    "])\n",
    "\n",
    "# Dataset (same)\n",
    "train_dataset = SSDDataset(train_dir, train_ann, transform=ssd_transform)\n",
    "val_dataset = SSDDataset(val_dir, val_ann, transform=ssd_transform)\n",
    "\n",
    "# DataLoader\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "537d91a2",
   "metadata": {},
   "source": [
    "Step 4: Initialize SSD300 Model and Set Up Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1bde753",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models.detection import ssd300_vgg16\n",
    "from torchvision.models.detection.ssd import SSDClassificationHead\n",
    "from torchvision.models.detection.ssd import SSD300_VGG16_Weights\n",
    "import torch\n",
    "\n",
    "# Define device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load pretrained SSD model\n",
    "weights = SSD300_VGG16_Weights.DEFAULT\n",
    "model = ssd300_vgg16(weights=weights)\n",
    "\n",
    "# SSD300 feature map configuration\n",
    "in_channels = [512, 1024, 512, 256, 256, 256]\n",
    "num_anchors = [4, 6, 6, 6, 4, 4]\n",
    "num_classes = len(train_dataset.cat_id_to_index) + 1  # +1 for background\n",
    "\n",
    "# Replace classification head\n",
    "model.head.classification_head = SSDClassificationHead(\n",
    "    in_channels=in_channels,\n",
    "    num_anchors=num_anchors,\n",
    "    num_classes=num_classes\n",
    ")\n",
    "\n",
    "# Move model to device\n",
    "model.to(device)\n",
    "print(f\" SSD300 model loaded with {num_classes} classes on {device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d8e984d",
   "metadata": {},
   "source": [
    "Step 5: Train SSD300 for 30 Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4567dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Set model to train mode\n",
    "model.train()\n",
    "\n",
    "# Define optimizer\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = optim.SGD(params, lr=0.001, momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "# Train for 30 epochs\n",
    "num_epochs = 30\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\nEpoch [{epoch+1}/{num_epochs}]\")\n",
    "    epoch_loss = 0.0\n",
    "\n",
    "    for images, targets in tqdm(train_loader):\n",
    "        images = [img.to(device) for img in images]\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        # Forward and backward pass\n",
    "        loss_dict = model(images, targets)\n",
    "        loss = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1} Loss: {epoch_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0adddac",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = r\"C:/Users/admin/Downloads/Code/ObjectDetection/ssdOutput\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "model_path = os.path.join(output_dir, \"ssd_model.pth\")\n",
    "torch.save(model.state_dict(), model_path)\n",
    "print(f\"Model saved to {model_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c945582c",
   "metadata": {},
   "source": [
    "Step 6: SSD300 Video Reasoning and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ec4fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from collections import Counter\n",
    "\n",
    "# Load model\n",
    "model.eval()\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "model.to(device)\n",
    "\n",
    "# Category mapping\n",
    "category_names = [cat['name'] for cat in train_dataset.categories]\n",
    "idx_to_name = {idx: name for idx, name in enumerate(category_names)}\n",
    "\n",
    "# Helper: resize + convert frame\n",
    "def preprocess_frame(frame):\n",
    "    image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    image = Image.fromarray(image)\n",
    "    image_resized = image.resize((300, 300))\n",
    "    tensor = transforms.ToTensor()(image_resized).unsqueeze(0).to(device)\n",
    "    return tensor\n",
    "\n",
    "# Helper: scale boxes back to original size\n",
    "def rescale_boxes(boxes, orig_size, input_size=(300, 300)):\n",
    "    orig_h, orig_w = orig_size\n",
    "    input_w, input_h = input_size\n",
    "    scale_w, scale_h = orig_w / input_w, orig_h / input_h\n",
    "    boxes[:, 0::2] *= scale_w\n",
    "    boxes[:, 1::2] *= scale_h\n",
    "    return boxes\n",
    "\n",
    "# Video detection loop\n",
    "video_dir = r\"C:/Users/admin/Downloads/Code/ObjectDetection/videos\"\n",
    "video_files = [f for f in os.listdir(video_dir) if f.endswith(\".mp4\")]\n",
    "\n",
    "for video_file in video_files:\n",
    "    video_path = os.path.join(video_dir, video_file)\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Failed to open {video_file}\")\n",
    "        continue\n",
    "\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "    output_path = os.path.join(output_dir, f\"annotated_{video_file}\")\n",
    "    out = cv2.VideoWriter(output_path, cv2.VideoWriter_fourcc(*\"mp4v\"), fps, (width, height))\n",
    "\n",
    "    frame_count = 0\n",
    "    detected_classes = []\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame_count += 1\n",
    "\n",
    "        tensor = preprocess_frame(frame)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model(tensor)[0]\n",
    "\n",
    "        boxes = output['boxes'].detach().cpu()\n",
    "        scores = output['scores'].detach().cpu()\n",
    "        labels = output['labels'].detach().cpu()\n",
    "\n",
    "        keep = (scores >= 0.05)\n",
    "        boxes = boxes[keep]\n",
    "        scores = scores[keep]\n",
    "        labels = labels[keep]\n",
    "\n",
    "        if boxes.size(0) == 0:\n",
    "            out.write(frame)\n",
    "            continue\n",
    "\n",
    "        boxes = rescale_boxes(boxes.clone(), (height, width)).int()\n",
    "\n",
    "        for box, score, label in zip(boxes, scores, labels):\n",
    "            x1, y1, x2, y2 = box.tolist()\n",
    "            cls_id = int(label.item())\n",
    "            name = idx_to_name.get(cls_id, \"Unknown\")\n",
    "            color = tuple(np.random.randint(0, 255, 3).tolist())\n",
    "            text = f\"{name} {score:.2f}\"\n",
    "\n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n",
    "            cv2.putText(frame, text, (x1, max(y1 - 10, 10)), cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n",
    "            detected_classes.append(cls_id)\n",
    "\n",
    "        out.write(frame)\n",
    "\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    print(f\" Saved annotated video to: {output_path}\")\n",
    "    if len(detected_classes) == 0:\n",
    "        print(\" No objects detected in this video.\")\n",
    "    else:\n",
    "        stats = Counter(detected_classes)\n",
    "        for cid, count in stats.items():\n",
    "            print(f\" - {idx_to_name.get(cid, 'Unknown')}: {count} detections\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c94233e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single-frame debug (optional)\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "ret, frame = cap.read()\n",
    "cap.release()\n",
    "\n",
    "tensor = preprocess_frame(frame)\n",
    "with torch.no_grad():\n",
    "    output = model(tensor)[0]\n",
    "\n",
    "boxes = output['boxes'].cpu()\n",
    "scores = output['scores'].cpu()\n",
    "labels = output['labels'].cpu()\n",
    "\n",
    "# Filter low scores\n",
    "keep = scores >= 0.25\n",
    "boxes = boxes[keep]\n",
    "scores = scores[keep]\n",
    "labels = labels[keep]\n",
    "\n",
    "boxes = rescale_boxes(boxes.clone(), (frame.shape[0], frame.shape[1])).int()\n",
    "\n",
    "# Visualize\n",
    "for box, score, label in zip(boxes, scores, labels):\n",
    "    x1, y1, x2, y2 = box.tolist()\n",
    "    name = idx_to_name.get(label.item(), \"Unknown\")\n",
    "    color = tuple(np.random.randint(0, 255, 3).tolist())\n",
    "    cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n",
    "    cv2.putText(frame, f\"{name} {score:.2f}\", (x1, max(y1 - 10, 10)),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n",
    "\n",
    "# Show with matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.imshow(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
